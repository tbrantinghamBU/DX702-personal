{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fcac30cc",
   "metadata": {},
   "source": [
    "# Homework Reflection 5-8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261b8d73",
   "metadata": {},
   "source": [
    "## Week 5: Simulated Dataset\n",
    "\n",
    "- Lightning storms frighten away deer and bears, decreasing their population, and cause flowers to grow, increasing their population.\n",
    "\n",
    "- Bears eat deer, decreasing their population.\n",
    "\n",
    "- Deer eat flowers, decreasing their population.\n",
    "\n",
    "- Write a dataset that simulates this situation.  (Show the code.) Include noise / randomness in all cases.\n",
    "\n",
    "- Identify a backdoor path with one or more confounders for the relationship between deer and flowers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2d79223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   lightning      bears       deer     flowers\n",
      "0   5.993428  42.211210  66.528361   65.845487\n",
      "1   4.723471  43.326958  70.528720   64.509123\n",
      "2   6.295377  37.588137  67.800766   79.148543\n",
      "3   8.046060  31.967070  70.407568  102.199920\n",
      "4   4.531693  43.031283  62.218745   94.723121\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "np.random.seed(42)  # reproducibility\n",
    "n = 1000  # number of observations\n",
    "\n",
    "lightning = np.random.normal(loc=5, scale=2, size=n)\n",
    "bears = 50 - 2*lightning + np.random.normal(0, 3, n)\n",
    "deer = 100 - 1.5*lightning - 0.5*bears + np.random.normal(0, 5, n)\n",
    "flowers = 200 + 3*lightning - 2*deer + np.random.normal(0, 10, n)\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"lightning\": lightning,\n",
    "    \"bears\": bears,\n",
    "    \"deer\": deer,\n",
    "    \"flowers\": flowers\n",
    "})\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ccdde87",
   "metadata": {},
   "source": [
    "## Week 6\n",
    "\n",
    "1. What is a potential problem with computing the Marginal Treatment Effect simply by comparing each untreated item to its counterfactual and taking the maximum difference?  (Hint: think of statistics here.  Consider that only the most extreme item ends up being used to estimate the MTE.  That's not necessarily a bad thing; the MTE is supposed to come from the untreated item that will produce the maximum effect.  But there is nevertheless a problem.)\n",
    "Possible answer: We are likely to find the item with the most extreme difference, which may be high simply due to randomness.\n",
    "(Please explain / justify this answer, or give a different one if you can think of one.)\n",
    "\n",
    "2. Propose a solution that remedies this problem and write some code that implements your solution.  It's very important here that you clearly explain what your solution will do.\n",
    "Possible answer: maybe we could take the 90th percentile of the treatment effect and use it as a proxy for the Marginal Treatment Effect.\n",
    "(Either code this answer or choose a different one.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "26e771a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MTE proxy (90th percentile): 1.9280\n",
      "Bootstrap 90th percentile CI (90% central): [1.8962, 1.9566]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "def untreated_effects(df):\n",
    "    treated = df[df['X'] == 1].reset_index(drop=True)\n",
    "    untreated = df[df['X'] == 0].reset_index(drop=True)\n",
    "\n",
    "    nn = NearestNeighbors(n_neighbors=1)\n",
    "    nn.fit(treated[['Z']])\n",
    "    distances, indices = nn.kneighbors(untreated[['Z']])\n",
    "\n",
    "    matched_treated = treated.iloc[indices.flatten()].reset_index(drop=True)\n",
    "    te_untreated = matched_treated['Y'].values - untreated['Y'].values  # TE for untreated items\n",
    "    return te_untreated\n",
    "\n",
    "def mte_quantile(te_array, q=0.90):\n",
    "    return np.quantile(te_array, q)\n",
    "\n",
    "def bootstrap_ci(te_array, q=0.90, n_boot=1000, seed=42):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    n = len(te_array)\n",
    "    qs = []\n",
    "    for _ in range(n_boot):\n",
    "        sample = te_array[rng.integers(0, n, size=n)]\n",
    "        qs.append(np.quantile(sample, q))\n",
    "    lower, upper = np.quantile(qs, [0.05, 0.95])\n",
    "    return lower, upper\n",
    "\n",
    "# --- Run estimation ---\n",
    "df = pd.read_csv(\"homework_6.1.csv\")\n",
    "te_untreated = untreated_effects(df)\n",
    "mte_90 = mte_quantile(te_untreated, q=0.90)\n",
    "ci_low, ci_high = bootstrap_ci(te_untreated, q=0.90, n_boot=1000, seed=42)\n",
    "\n",
    "print(f\"MTE proxy (90th percentile): {mte_90:.4f}\")\n",
    "print(f\"Bootstrap 90th percentile CI (90% central): [{ci_low:.4f}, {ci_high:.4f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07c2d75",
   "metadata": {},
   "source": [
    "## Week 7 \n",
    "\n",
    "1. Create a linear regression model involving a confounder that is left out of the model.  Show whether the true correlation between X and Y is overestimated, underestimated, or neither.  Explain in words why this is the case for the given coefficients you have chosen.\n",
    "\n",
    "2. Perform a linear regression analysis in which one of the coefficients is zero, e.g.\n",
    "•\tW = [noise]\n",
    "•\tX = [noise]\n",
    "•\tY = 2 * X + [noise]\n",
    "•\tAnd compute the p-value of a coefficient - in this case, the coefficient of W.  \n",
    "•\t(This is the likelihood that the estimated coefficient would be as high or low as it is, given that the actual coefficient is zero.)\n",
    "•\tIf the p-value is less than 0.05, this ordinarily means that we judge the coefficient to be nonzero (incorrectly, in this case.)\n",
    "•\tRun the analysis 1000 times and report the best (smallest) p-value.  \n",
    "•\tIf the p-value is less than 0.05, does this mean the coefficient actually is nonzero?  What is the problem with repeating the analysis?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c5e09bce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Omitting Z (biased): const   -0.033439\n",
      "X        3.410282\n",
      "dtype: float64\n",
      "Including Z (unbiased): const   -0.012768\n",
      "X        2.026782\n",
      "Z        2.986459\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate data\n",
    "n = 10000\n",
    "Z = np.random.normal(0, 1, n)\n",
    "u = np.random.normal(0, 1, n)\n",
    "X = 1.5 * Z + u\n",
    "error = np.random.normal(0, 1, n)\n",
    "Y = 2 * X + 3 * Z + error\n",
    "\n",
    "df = pd.DataFrame({'Y': Y, 'X': X, 'Z': Z})\n",
    "\n",
    "# Regression omitting Z\n",
    "X_only = sm.add_constant(df[['X']])\n",
    "model_omit = sm.OLS(df['Y'], X_only).fit()\n",
    "\n",
    "# Regression including Z\n",
    "XZ = sm.add_constant(df[['X', 'Z']])\n",
    "model_full = sm.OLS(df['Y'], XZ).fit()\n",
    "\n",
    "print(\"Omitting Z (biased):\", model_omit.params)\n",
    "print(\"Including Z (unbiased):\", model_full.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc042f4",
   "metadata": {},
   "source": [
    "## Week 7 Cont\n",
    "\n",
    "- Perform a linear regression analysis in which one of the coefficients is zero, e.g. •\tW = [noise] •\tX = [noise] •\tY = 2 * X + [noise] •\tAnd compute the p-value of a coefficient - in this case, the coefficient of W.\n",
    "•\t(This is the likelihood that the estimated coefficient would be as high or low as it is, given that the actual coefficient is zero.) •\tIf the p-value is less than 0.05, this ordinarily means that we judge the coefficient to be nonzero (incorrectly, in this case.) •\tRun the analysis 1000 times and report the best (smallest) p-value.\n",
    "•\tIf the p-value is less than 0.05, does this mean the coefficient actually is nonzero?  What is the problem with repeating the analysis?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9502af90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smallest p-value across 1000 runs: 0.000245\n",
      "Number of runs with p < 0.05: 55 (out of 1000)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "def run_once(n=500):\n",
    "    W = np.random.normal(0, 1, n)\n",
    "    X = np.random.normal(0, 1, n)\n",
    "    eps = np.random.normal(0, 1, n)\n",
    "    Y = 2 * X + eps  # W does not affect Y\n",
    "\n",
    "    df = pd.DataFrame({'Y': Y, 'X': X, 'W': W})\n",
    "    XW = sm.add_constant(df[['X', 'W']])\n",
    "    model = sm.OLS(df['Y'], XW).fit()\n",
    "    p_w = model.pvalues['W']  # p-value for coefficient on W\n",
    "    return p_w\n",
    "\n",
    "# Run 1000 simulations and record the smallest p-value\n",
    "p_values = [run_once(n=500) for _ in range(1000)]\n",
    "min_p = np.min(p_values)\n",
    "\n",
    "print(f\"Smallest p-value across 1000 runs: {min_p:.6f}\")\n",
    "# Optional: how many runs are \"significant\" at 0.05\n",
    "sig_count = np.sum(np.array(p_values) < 0.05)\n",
    "print(f\"Number of runs with p < 0.05: {sig_count} (out of 1000)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a664f97",
   "metadata": {},
   "source": [
    "# Week 8\n",
    "\n",
    "I just copy-pasted all my code from my Homework Notebook here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e1fe9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy.spatial.distance import mahalanobis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7107d3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('homework_8.1.csv', index_col=0)\n",
    "df2 = pd.read_csv('homework_8.2.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6474ed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated ATE using IPW: 2.2743275711428588\n"
     ]
    }
   ],
   "source": [
    "# Estimate propensity scores P(X=1 | Z)\n",
    "logit = LogisticRegression(solver='liblinear')\n",
    "logit.fit(df1[['Z']], df1['X'])\n",
    "df1['propensity'] = logit.predict_proba(df1[['Z']])[:, 1]\n",
    "\n",
    "# Compute the inverse probability weights\n",
    "df1['weight'] = np.where( df1['X'] == 1,\n",
    "                          1 / df1['propensity'], \n",
    "                          1 / (1 - df1['propensity']))\n",
    "\n",
    "# Weighted means of Y for treated and control groups\n",
    "treated_mean = np.average(df1.loc[df1['X'] == 1, 'Y'], weights=df1.loc[df1['X'] == 1, 'weight'])\n",
    "control_mean = np.average(df1.loc[df1['X'] == 0, 'Y'], weights=df1.loc[df1['X'] == 0, 'weight'])\n",
    "\n",
    "# ATE\n",
    "ate = treated_mean - control_mean\n",
    "print(f'Estimated ATE using IPW: {ate}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e603a1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Propensity scores of the first three items:\n",
      "[0.84011371 0.58464597 0.71108245]\n"
     ]
    }
   ],
   "source": [
    "print(\"Propensity scores of the first three items:\")\n",
    "print(df1['propensity'].head(3).values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808db91f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated ATE using Nearest Neighbor Matching: 3.4376789979126094\n"
     ]
    }
   ],
   "source": [
    "# Split into treted and control groups\n",
    "treated = df2[df2['X'] == 1].reset_index(drop=True)\n",
    "control = df2[df2['X'] == 0].reset_index(drop=True)\n",
    "\n",
    "# Build the covariance matrix for Mahalanobis distance\n",
    "Z = df2[['Z1', 'Z2']].values\n",
    "cov = np.cov(Z.T)\n",
    "inv_cov = np.linalg.inv(cov)\n",
    "\n",
    "# Find the nearest control for each treated unit\n",
    "matches = []\n",
    "\n",
    "for i in range(len(treated)):\n",
    "    z_t = treated.loc[i, ['Z1', 'Z2']].values\n",
    "    dists = control[['Z1', 'Z2']].apply(lambda row: mahalanobis(z_t, row.values, inv_cov), axis=1)\n",
    "    j = dists.idxmin()\n",
    "    matched_control = control.loc[j]\n",
    "    matches.append({'treated_Y': treated.loc[i, 'Y'],\n",
    "                    'control_Y': matched_control['Y'],\n",
    "                    'diff': treated.loc[i, 'Y'] - matched_control['Y']})\n",
    "    \n",
    "matches_df = pd.DataFrame(matches)\n",
    "\n",
    "ate = matches_df['diff'].mean()\n",
    "print(f'Estimated ATE using Nearest Neighbor Matching: {ate}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489d08c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treated unit with least common support:\n",
      "  Z1 = 2.69622405256358, Z2 = 0.5381554886023228\n",
      "Nearest control unit:\n",
      "  Z1 = 1.5199948607657727, Z2 = -1.2822079376259403\n",
      "Mahalanobis distance = 1.3830045328325056\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import mahalanobis\n",
    "\n",
    "# Load your dataset\n",
    "df2 = pd.read_csv(\"homework_8.2.csv\")\n",
    "\n",
    "# Split into treated and control groups\n",
    "treated = df2[df2['X'] == 1].reset_index(drop=True)\n",
    "control = df2[df2['X'] == 0].reset_index(drop=True)\n",
    "\n",
    "# Build the covariate matrix (Z1, Z2)\n",
    "Z = df2[['Z1', 'Z2']].values\n",
    "cov = np.cov(Z.T)\n",
    "inv_cov = np.linalg.inv(cov)\n",
    "\n",
    "# For each treated unit, find its nearest control and record the distance\n",
    "results = []\n",
    "for i in range(len(treated)):\n",
    "    z_t = treated.loc[i, ['Z1', 'Z2']].values\n",
    "    dists = control[['Z1', 'Z2']].apply(\n",
    "        lambda row: mahalanobis(z_t, row.values, inv_cov), axis=1\n",
    "    )\n",
    "    j = dists.idxmin()\n",
    "    results.append({\n",
    "        'treated_index': i,\n",
    "        'treated_Z1': treated.loc[i, 'Z1'],\n",
    "        'treated_Z2': treated.loc[i, 'Z2'],\n",
    "        'control_index': j,\n",
    "        'control_Z1': control.loc[j, 'Z1'],\n",
    "        'control_Z2': control.loc[j, 'Z2'],\n",
    "        'min_distance': dists.min()\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Find the treated unit with the *largest* minimum distance (least common support)\n",
    "worst_match = results_df.loc[results_df['min_distance'].idxmax()]\n",
    "\n",
    "print(\"Treated unit with least common support:\")\n",
    "print(f\"  Z1 = {worst_match['treated_Z1']}, Z2 = {worst_match['treated_Z2']}\")\n",
    "print(\"Nearest control unit:\")\n",
    "print(f\"  Z1 = {worst_match['control_Z1']}, Z2 = {worst_match['control_Z2']}\")\n",
    "print(f\"Mahalanobis distance = {worst_match['min_distance']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
